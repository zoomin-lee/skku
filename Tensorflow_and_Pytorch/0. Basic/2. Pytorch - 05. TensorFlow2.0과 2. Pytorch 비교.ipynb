{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras import datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "batch_size = 64\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "dropout_rate = 0.7\n",
    "\n",
    "input_shape = (28, 28, 1)\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x, train_y), (test_x, test_y) = datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x[..., tf.newaxis]\n",
    "test_x = test_x[..., tf.newaxis]\n",
    "\n",
    "train_x = train_x / 255.\n",
    "test_x = test_x / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(input_shape)\n",
    "net = layers.Conv2D(32, (3, 3), padding='SAME')(inputs)\n",
    "net = layers.Activation('relu')(net)\n",
    "net = layers.Conv2D(32, (3, 3), padding='SAME')(net)\n",
    "net = layers.Activation('relu')(net)\n",
    "net = layers.MaxPooling2D(pool_size=(2, 2))(net)\n",
    "net = layers.Dropout(dropout_rate)(net)\n",
    "\n",
    "net = layers.Conv2D(64, (3, 3), padding='SAME')(net)\n",
    "net = layers.Activation('relu')(net)\n",
    "net = layers.Conv2D(64, (3, 3), padding='SAME')(net)\n",
    "net = layers.Activation('relu')(net)\n",
    "net = layers.MaxPooling2D(pool_size=(2, 2))(net)\n",
    "net = layers.Dropout(dropout_rate)(net)\n",
    "\n",
    "net = layers.Flatten()(net)\n",
    "net = layers.Dense(512)(net)\n",
    "net = layers.Activation('relu')(net)\n",
    "net = layers.Dropout(dropout_rate)(net)\n",
    "net = layers.Dense(num_classes)(net)\n",
    "net = layers.Activation('softmax')(net)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=net, name='Basic_CNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model is the full model w/o custom layers\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),  # Optimization\n",
    "              loss='sparse_categorical_crossentropy',  # Loss Function \n",
    "              metrics=['accuracy'])  # Metrics / Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 162s 172ms/step - loss: 0.3916 - accuracy: 0.8728\n",
      "157/157 [==============================] - 7s 40ms/step - loss: 0.0685 - accuracy: 0.9763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06854621320962906, 0.9763000011444092]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y,   \n",
    "             batch_size=batch_size, \n",
    "             shuffle=True)\n",
    "\n",
    "model.evaluate(test_x, test_y, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "\n",
    "lr = 0.001\n",
    "momentum = 0.5\n",
    "\n",
    "batch_size = 64\n",
    "test_batch_size = 64\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "no_cuda = False\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('dataset', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('dataset', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=test_batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jumin\\anaconda3\\envs\\fastcampus\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.299172\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.233123\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.125054\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.954095\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.746468\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.243029\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.026445\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.617744\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.717772\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.411972\n",
      "\n",
      "Test set: Average loss: 0.4822, Accuracy: 8671/10000 (87%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.718479\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.433093\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.361737\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.419212\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.372070\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.486507\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.337888\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.441602\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.307979\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.358055\n",
      "\n",
      "Test set: Average loss: 0.3068, Accuracy: 9059/10000 (91%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.310498\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.316001\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.535031\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.515734\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.190066\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.252311\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.248714\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.189229\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.169730\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.395538\n",
      "\n",
      "Test set: Average loss: 0.2258, Accuracy: 9339/10000 (93%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.225045\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.181498\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.328304\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.116514\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.206793\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.161030\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.149706\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.213237\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.153016\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.061648\n",
      "\n",
      "Test set: Average loss: 0.1900, Accuracy: 9457/10000 (95%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.154742\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.209430\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.317660\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.246277\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.097571\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.163521\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.191873\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.334213\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.090189\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.183962\n",
      "\n",
      "Test set: Average loss: 0.1554, Accuracy: 9553/10000 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    # Train Mode\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()  # backpropagation 계산하기 전에 0으로 기울기 계산\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)  # https://pytorch.org/docs/stable/nn.html#nll-loss\n",
    "        loss.backward()  # 계산한 기울기를 \n",
    "        optimizer.step()  \n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    \n",
    "    # Test mode\n",
    "    model.eval()  # batch norm이나 dropout 등을 train mode 변환\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():  # autograd engine, 즉 backpropagatin이나 gradient 계산 등을 꺼서 memory usage를 줄이고 속도를 높임\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()  # pred와 target과 같은지 확인\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
